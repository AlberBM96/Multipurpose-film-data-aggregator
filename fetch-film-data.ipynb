{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57626c47",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.005438,
     "end_time": "2023-07-19T09:52:09.939960",
     "exception": false,
     "start_time": "2023-07-19T09:52:09.934522",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fetch film data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a62fff",
   "metadata": {
    "papermill": {
     "duration": 0.004713,
     "end_time": "2023-07-19T09:52:09.949664",
     "exception": false,
     "start_time": "2023-07-19T09:52:09.944951",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8290855f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T09:52:09.961585Z",
     "iopub.status.busy": "2023-07-19T09:52:09.961125Z",
     "iopub.status.idle": "2023-07-19T09:52:10.178046Z",
     "shell.execute_reply": "2023-07-19T09:52:10.176598Z"
    },
    "papermill": {
     "duration": 0.226536,
     "end_time": "2023-07-19T09:52:10.181195",
     "exception": false,
     "start_time": "2023-07-19T09:52:09.954659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import datetime as dt\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c99ac63",
   "metadata": {
    "papermill": {
     "duration": 0.004737,
     "end_time": "2023-07-19T09:52:10.191119",
     "exception": false,
     "start_time": "2023-07-19T09:52:10.186382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Choose data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c786d229",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T09:52:10.203073Z",
     "iopub.status.busy": "2023-07-19T09:52:10.202431Z",
     "iopub.status.idle": "2023-07-19T09:52:10.208396Z",
     "shell.execute_reply": "2023-07-19T09:52:10.207206Z"
    },
    "papermill": {
     "duration": 0.014526,
     "end_time": "2023-07-19T09:52:10.210621",
     "exception": false,
     "start_time": "2023-07-19T09:52:10.196095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_type = \"rating\" # Choose between \"rating\", \"popular\" and \"list\"\n",
    "if source_type == \"rating\":\n",
    "    list_url = \"https://letterboxd.com/films/ajax/by/rating/\"\n",
    "elif source_type == \"popular\":\n",
    "    list_url = \"https://letterboxd.com/films/ajax/popular/\"\n",
    "elif source_type == \"list\":\n",
    "    # URL of the target Letterboxd list\n",
    "    pass \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f544d81b",
   "metadata": {
    "papermill": {
     "duration": 0.004795,
     "end_time": "2023-07-19T09:52:10.220633",
     "exception": false,
     "start_time": "2023-07-19T09:52:10.215838",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Choose how many pages to scrap, or find the amount of pages of a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d54a137",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T09:52:10.233421Z",
     "iopub.status.busy": "2023-07-19T09:52:10.232370Z",
     "iopub.status.idle": "2023-07-19T09:52:10.243357Z",
     "shell.execute_reply": "2023-07-19T09:52:10.240703Z"
    },
    "papermill": {
     "duration": 0.020702,
     "end_time": "2023-07-19T09:52:10.246412",
     "exception": false,
     "start_time": "2023-07-19T09:52:10.225710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if source_type in [\"rating\", \"popular\"]:\n",
    "    total_pages = 10\n",
    "elif source_type == \"list\":\n",
    "    session = requests.session()\n",
    "    response = session.get(f\"{list_url}?esiAllowFilters=true\")\n",
    "    cookies = requests.utils.dict_from_cookiejar(session.cookies)\n",
    "    # Use cookies to enable search filters\n",
    "    # Filters; combine into a string with \"%20\" in between options\n",
    "    cookies['filmFilter'] = 'hide-tv%20hide-shorts%20hide-unreleased%20hide-docs'\n",
    "    Path(\"cookies.json\").write_text(json.dumps(cookies))\n",
    "\n",
    "    session = requests.session()\n",
    "    cookies = json.loads(Path(\"cookies.json\").read_text())\n",
    "    cookies = requests.utils.cookiejar_from_dict(cookies)\n",
    "    session.cookies.update(cookies)\n",
    "    response = session.get(f\"{list_url}?esiAllowFilters=true\")\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    total_pages = int(soup.find(\"div\", class_=\"paginate-pages\").find_all(\"a\", href=True)[-1].text)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54550de",
   "metadata": {
    "papermill": {
     "duration": 0.005768,
     "end_time": "2023-07-19T09:52:10.257643",
     "exception": false,
     "start_time": "2023-07-19T09:52:10.251875",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fetch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b024ace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T09:52:10.271350Z",
     "iopub.status.busy": "2023-07-19T09:52:10.270614Z",
     "iopub.status.idle": "2023-07-19T09:52:11.202505Z",
     "shell.execute_reply": "2023-07-19T09:52:11.200960Z"
    },
    "papermill": {
     "duration": 0.941668,
     "end_time": "2023-07-19T09:52:11.204907",
     "exception": true,
     "start_time": "2023-07-19T09:52:10.263239",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cookies.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page_n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, total_pages\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      7\u001b[0m     session \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39msession()\n\u001b[0;32m----> 8\u001b[0m     cookies \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcookies.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m     cookies \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcookiejar_from_dict(cookies)\n\u001b[1;32m     10\u001b[0m     session\u001b[38;5;241m.\u001b[39mcookies\u001b[38;5;241m.\u001b[39mupdate(cookies)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/pathlib.py:1134\u001b[0m, in \u001b[0;36mPath.read_text\u001b[0;34m(self, encoding, errors)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;124;03mOpen the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1134\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/pathlib.py:1119\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1118\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cookies.json'"
     ]
    }
   ],
   "source": [
    "film_df = pd.DataFrame(columns=[\"Title\", \"Year\", \"ReleaseDate\", \"Director\", \"Writer\", \"Cast\", \"Runtime\", \"Genre\", \"Country\", \"Language\", \"Budget\", \"BoxOffice\",\n",
    "                                \"IMDbScore\", \"IMDbVotes\", \"IMDbReviews\", \"IMDbURL\", \n",
    "                                \"LetterboxdScore\", \"LetterboxdVotes\", \"LetterboxdWatches\", \"LetterboxdReviews\", \"LetterboxdURL\",\n",
    "                                \"MetacriticCriticScore\", \"MetacriticCriticReviews\", \"MetacriticUserScore\", \"MetacriticUserVotes\", \"MetacriticUserReviews\", \"MetacriticURL\"])\n",
    "\n",
    "for page_n in range(1, total_pages+1):\n",
    "    session = requests.session()\n",
    "    cookies = json.loads(Path(\"cookies.json\").read_text())\n",
    "    cookies = requests.utils.cookiejar_from_dict(cookies)\n",
    "    session.cookies.update(cookies)\n",
    "    response = session.get(f\"{list_url}page/{page_n}/?esiAllowFilters=true\")\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    film_list = soup.find_all(\"li\", class_=\"listitem\")\n",
    "\n",
    "    for film_item in film_list:\n",
    "        tmp_data = pd.Series(data=None, dtype=object)\n",
    "        for col in film_df.columns:\n",
    "            tmp_data[col] = np.nan\n",
    "        # Letterboxd\n",
    "        lbxd_film_handle = film_item.find(\"div\")['data-film-slug']\n",
    "        lbxd_full_url = f\"https://letterboxd.com{lbxd_film_handle}\"\n",
    "        tmp_data[\"LetterboxdURL\"] = str(lbxd_full_url)\n",
    "        response = requests.get(lbxd_full_url, headers={'User-agent': 'Mozilla/5.0'}, timeout=30)\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        \n",
    "        try:\n",
    "            title = soup.find(\"div\", id=\"content\").find(\"section\", id=\"featured-film-header\").find(\"h1\").text\n",
    "            tmp_data[\"Title\"] = str(title)\n",
    "        except:\n",
    "            print(\"\\tTitle not found\")\n",
    "            \n",
    "        try:\n",
    "            year = soup.find(\"section\", id=\"featured-film-header\").find(\"a\").text\n",
    "            tmp_data[\"Year\"] = int(year)\n",
    "        except:\n",
    "            print(\"\\tYear not found\")\n",
    "            \n",
    "        try:\n",
    "            director = soup.find(\"meta\", attrs={\"name\":\"twitter:data1\"})['content']\n",
    "            tmp_data[\"Director\"] = str(director)\n",
    "        except:\n",
    "            print(\"\\tDirector not found\")\n",
    "\n",
    "        try:\n",
    "            lbxd_rating = soup.find(\"meta\", attrs={\"name\":\"twitter:data2\"})['content'].split()[0]\n",
    "            tmp_data[\"LetterboxdScore\"] = float(lbxd_rating)\n",
    "        except:\n",
    "            print(\"\\tLetterboxd rating not found\")\n",
    "            \n",
    "        try:\n",
    "            imdb_url = soup.find(\"a\", attrs={\"data-track-action\": \"IMDb\"}, href=True)['href']\n",
    "            imdb_url = imdb_url.replace(\"maindetails\",\"\")\n",
    "            tmp_data[\"IMDbURL\"] = str(imdb_url)\n",
    "        except:\n",
    "            print(\"\\tIMDb URL not found\")\n",
    "            \n",
    "        try:\n",
    "            runtime = soup.find(\"p\", class_=\"text-link text-footer\").text.split()[0]\n",
    "            tmp_data[\"Runtime\"] = int(runtime)\n",
    "        except:\n",
    "            print(\"\\tRuntime not found\")\n",
    "            \n",
    "        try:\n",
    "            premiere_date = soup.find(\"div\", class_=\"release-table -bydate\").find(\"h5\").text\n",
    "            tmp_data[\"ReleaseDate\"] = str(premiere_date)\n",
    "        except:\n",
    "            print(\"\\tRelease date not found\")\n",
    "        \n",
    "        try:\n",
    "            idx = soup.prettify().find('\"ratingCount\":')\n",
    "            idx2 = soup.prettify().find(',\"worstRating')\n",
    "            lbxd_ratings = float(soup.prettify()[idx:idx2].replace('\"ratingCount\":', ''))\n",
    "            tmp_data[\"LetterboxdVotes\"] = int(lbxd_ratings)\n",
    "        except:\n",
    "            print(\"\\tLetterboxd votes not found\")\n",
    "            \n",
    "        response = requests.get(f\"{lbxd_full_url}members\", headers={'User-agent': 'Mozilla/5.0'}, timeout=30)\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        try:\n",
    "            lbxd_watches = soup.find(\"li\", class_=\"js-route-watches\").find(\"a\")['title'].split()\n",
    "            tmp_data[\"LetterboxdWatches\"] = int(lbxd_watches[0].replace(\",\",\"\"))\n",
    "        except:\n",
    "            print(\"\\tLetterboxd watches not found\")\n",
    "        \n",
    "        try:\n",
    "            lbxd_reviews = soup.find(\"li\", class_=\"js-route-reviews\").find(\"a\")['title'].split()\n",
    "            tmp_data[\"LetterboxdReviews\"] = int(lbxd_reviews[0].replace(\",\",\"\"))\n",
    "        except:\n",
    "            print(\"\\tLetterboxd reviews not found\")\n",
    "            \n",
    "        # IMDb\n",
    "        response = requests.get(imdb_url, headers={'User-agent': 'Mozilla/5.0'}, timeout=30)\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        try:\n",
    "            idx = soup.prettify().find('{\"@type\":\"AggregateRating\"')\n",
    "            idx2 = soup.prettify().find(',\"contentRating\"')\n",
    "            tmp = soup.prettify()[idx:idx2]\n",
    "            tmp2 = ast.literal_eval(tmp)\n",
    "            IMDbScore = tmp2['ratingValue']\n",
    "            tmp_data[\"IMDbScore\"] = float(IMDbScore)\n",
    "            IMDbVotes = tmp2['ratingCount']\n",
    "            tmp_data[\"IMDbVotes\"] = int(IMDbVotes)\n",
    "        except:\n",
    "            print(\"\\tIMDb rating not found\")\n",
    "            \n",
    "        try:\n",
    "            tmp = soup.find(\"li\", attrs={\"data-testid\":\"title-boxoffice-budget\"})\n",
    "            tmp2 = tmp.find(\"span\", class_=\"ipc-metadata-list-item__list-content-item\").text\n",
    "            if \"$\" in tmp2:\n",
    "                budget = float(tmp2.replace(\"$\", \"\").replace(\"(estimated)\", \"\").replace(\",\", \"\"))\n",
    "            elif \"€\" in tmp2:\n",
    "                budget = 1.11 * float(tmp2.replace(\"€\", \"\").replace(\"(estimated)\", \"\").replace(\",\", \"\"))\n",
    "            elif \"£\" in tmp2:\n",
    "                budget = 1.27 * float(tmp2.replace(\"£\", \"\").replace(\"(estimated)\", \"\").replace(\",\", \"\"))\n",
    "            elif \"¥\" in tmp2:\n",
    "                budget = 0.007 * float(tmp2.replace(\"¥\", \"\").replace(\"(estimated)\", \"\").replace(\",\", \"\"))\n",
    "            tmp_data[\"Budget\"] = budget\n",
    "        except:\n",
    "            print(\"\\tBudget not found\")\n",
    "            \n",
    "        try:\n",
    "            tmp = soup.find(\"li\", attrs={\"data-testid\":\"title-boxoffice-cumulativeworldwidegross\"})\n",
    "            tmp2 = tmp.find(\"span\", class_=\"ipc-metadata-list-item__list-content-item\").text\n",
    "            if \"$\" in tmp2:\n",
    "                box_office = float(tmp2.replace(\"$\", \"\").replace(\"(estimated)\", \"\").replace(\",\", \"\"))\n",
    "            elif \"€\" in tmp2:\n",
    "                box_office = 1.11 * float(tmp2.replace(\"€\", \"\").replace(\"(estimated)\", \"\").replace(\",\", \"\"))\n",
    "            elif \"£\" in tmp2:\n",
    "                box_office = 1.27 * float(tmp2.replace(\"£\", \"\").replace(\"(estimated)\", \"\").replace(\",\", \"\"))\n",
    "            elif \"¥\" in tmp2:\n",
    "                box_office = 0.007 * float(tmp2.replace(\"¥\", \"\").replace(\"(estimated)\", \"\").replace(\",\", \"\"))\n",
    "            tmp_data[\"BoxOffice\"] = box_office\n",
    "        except:\n",
    "            print(\"\\tBox office not found\")\n",
    "            \n",
    "        \n",
    "        response = requests.get(f\"{imdb_url}reviews/\", headers={'User-agent': 'Mozilla/5.0'}, timeout=30)\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        try:\n",
    "            tmp = [i.prettify() for i in soup.find_all(\"div\", class_=\"header\")]\n",
    "            tmp2 = \";\".join(tmp)\n",
    "            res = re.search('[0-9]+ Reviews', tmp2)\n",
    "            res2 = re.search('[0-9]+', res.group())\n",
    "            IMDbReviews = res2.group()\n",
    "            tmp_data[\"IMDbReviews\"] = IMDbReviews\n",
    "        except:\n",
    "            print(\"\\tIMDb reviews not found\")\n",
    "        \n",
    "        # Metacritic\n",
    "        response = requests.get(f\"{imdb_url}criticreviews\", headers={'User-agent': 'Mozilla/5.0'}, timeout=30)\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        try:\n",
    "            tmp = re.search(\"(?P<url>https?://www.metacritic[^\\s]+)\", soup.prettify()).group(\"url\")\n",
    "            metacritic_url = f\"{tmp.split('?ftag')[0]}/\"\n",
    "            #print(str(metacritic_url))\n",
    "            tmp_data[\"MetacriticURL\"] = str(metacritic_url)\n",
    "            response = requests.get(metacritic_url, headers={'User-agent': 'Mozilla/5.0'})\n",
    "            soup = BeautifulSoup(response.text)\n",
    "            try:\n",
    "                raw_scores = soup.find_all(\"a\", class_=\"metascore_anchor\")\n",
    "                tmp_data[\"MetacriticCriticScore\"] = float(raw_scores[0].text)/10.0\n",
    "                tmp_data[\"MetacriticUserScore\"] = float(raw_scores[1].text)\n",
    "            except:\n",
    "                print(\"\\tMetacritic scores not found\")\n",
    "            \n",
    "            try:\n",
    "                tmp = soup.find_all(\"a\", class_=\"data\", href=True)\n",
    "                for el in tmp:\n",
    "                    if 'user' in el['href']:\n",
    "                        metacritic_user_votes = re.search('[0-9]+', el.text).group()\n",
    "                    elif 'critic' in el['href']:\n",
    "                        metacritic_critic_reviews = re.search('[0-9]+', el.text).group()\n",
    "                tmp_data[\"MetacriticCriticReviews\"] = int(metacritic_critic_reviews)\n",
    "                tmp_data[\"MetacriticUserVotes\"] = int(metacritic_user_votes)\n",
    "            except:\n",
    "                print(\"\\tMetacritic reviews not found\")\n",
    "            \n",
    "            try:\n",
    "                tmp = soup.find_all(\"a\", class_=\"see_all\")\n",
    "                for el in tmp:\n",
    "                    if 'user' in el['href']:\n",
    "                        metacritic_user_reviews = re.search('[0-9]+', el.text).group()\n",
    "                    else:\n",
    "                        pass\n",
    "                tmp_data[\"MetacriticUserReviews\"] = int(metacritic_user_reviews)\n",
    "            except:\n",
    "                print(\"\\tMetacritic user reviews not found\")\n",
    "                \n",
    "            response = requests.get(f\"{metacritic_url}details\", headers={'User-agent': 'Mozilla/5.0'})\n",
    "            soup = BeautifulSoup(response.text)\n",
    "            try:\n",
    "                tmp = soup.find(\"table\", class_=\"details\").find(\"tr\", class_=\"genres\").text\n",
    "                genre = re.sub(\"\\n\", \"\", re.sub(\" +\", \" \", re.sub(\"Genres:\", \"\", re.sub(\"Genre:\", \"\", tmp))))\n",
    "                tmp_data[\"Genre\"] = str(genre)\n",
    "            except:\n",
    "                print(\"\\tGenre not found\")\n",
    "\n",
    "            try:\n",
    "                tmp = soup.find(\"table\", class_=\"details\").find(\"tr\", class_=\"countries\").text\n",
    "                country = re.sub(\"\\n\", \"\", re.sub(\" +\", \" \", re.sub(\"Countries:\", \"\", re.sub(\"Country:\", \"\", tmp))))\n",
    "                tmp_data[\"Country\"] = str(country)\n",
    "            except:\n",
    "                print(\"\\tCountry not found\")\n",
    "\n",
    "            try:\n",
    "                tmp = soup.find(\"table\", class_=\"details\").find(\"tr\", class_=\"languages\").text\n",
    "                language = re.sub(\"\\n\", \"\", re.sub(\" +\", \" \", re.sub(\"Languages:\", \"\", re.sub(\"Language:\", \"\", tmp))))\n",
    "                tmp_data[\"Language\"] = str(language)\n",
    "            except:\n",
    "                print(\"\\tLanguage not found\")\n",
    "\n",
    "            try:\n",
    "                tmp = soup.find_all(\"table\", class_=\"credits\")\n",
    "                director = []\n",
    "                writer = []\n",
    "                cast = []\n",
    "                for el in tmp:\n",
    "                    credits = el['summary'].lower()\n",
    "                    if 'director credits' in credits:\n",
    "                        for it in el.find_all(\"td\", class_=\"person\"):\n",
    "                            tmp = it.find(\"a\").text\n",
    "                            director.append(re.sub(\"\\n\", \"\", re.sub(\"  \", \"\", tmp)))\n",
    "                    elif 'writer credits' in credits:\n",
    "                        for it in el.find_all(\"td\", class_=\"person\"):\n",
    "                            tmp = it.find(\"a\").text\n",
    "                            writer.append(re.sub(\"\\n\", \"\", re.sub(\"  \", \"\", tmp)))\n",
    "                    elif 'principal cast credits' in credits:\n",
    "                        for it in el.find_all(\"td\", class_=\"person\"):\n",
    "                            tmp = it.find(\"a\").text\n",
    "                            cast.append(re.sub(\"\\n\", \"\", re.sub(\"  \", \"\", tmp)))\n",
    "                director = \",\".join(director)\n",
    "                tmp_data[\"Director\"] = str(director)\n",
    "                writer = \",\".join(writer)\n",
    "                tmp_data[\"Writer\"] = str(writer)\n",
    "                cast = \",\".join(cast)\n",
    "                tmp_data[\"Cast\"] = str(cast)\n",
    "            except:\n",
    "                print(\"\\tCast and crew not found\")\n",
    "        except:\n",
    "            print(\"\\tMetacritic URL not found\")\n",
    "        \n",
    "        film_df = pd.concat([film_df, tmp_data.to_frame().T], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda2c69",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Try and filter out unwanted entries (TV shows, music documentaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03669b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "film_df.drop(index=film_df[film_df[\"Genre\"] == \"Music\"].index, inplace=True)\n",
    "film_df.drop(index=film_df[film_df[\"Runtime\"] >= 250].index, inplace=True)\n",
    "film_df.dropna(axis=\"index\", subset=\"Runtime\", inplace=True)\n",
    "film_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a31eecc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Clean up country information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a412e5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang_dict = {\"AT\":\"Austria\", \"AU\":\"Australia\", \"BA\":\"Bosnia\", \"BE\":\"Belgium\", \"BG\":\"Bulgaria\", \"BR\":\"Brazil\", \n",
    "            \"CA\":\"Canada\", \"CH\":\"Switzerland\", \"CL\":\"Chile\", \"CN\":\"China\", \"CO\":\"Colombia\", \"CZ\":\"Czech Republic\", \n",
    "            \"DE\":\"Germany\", \"DK\":\"Denmark\", \"DZ\":\"Algeria\", \"ES\":\"Spain\", \"FI\":\"Finland\", \"FR\":\"France\", \n",
    "            \"GB\":\"United Kingdom\", \"GE\":\"Georgia\", \"GR\":\"Greece\", \"HK\":\"Hong Kong\", \"HU\":\"Hungary\", \n",
    "            \"IE\":\"Ireland\", \"IN\":\"India\", \"IR\":\"Iran\", \"IT\":\"Italy\", \"JPN\":\"Japan\", \"JP\":\"Japan\",  \n",
    "            \"KR\":\"South Korea\", \"LB\":\"Lebanon\", \"LU\": \"Luxembourg\", \"MA\":\"Macedonia\", \"MC\":\"Monaco\", \"MT\":\"Malta\", \"MX\":\"Mexico\", \n",
    "            \"NL\":\"Netherlands\", \"NO\":\"Norway\", \"NZ\":\"New Zealand\", \"PK\":\"Pakistan\", \"PL\":\"Poland\", \"PT\":\"Portugal\",\n",
    "            \"RO\":\"Romania\", \"RU\":\"Russia\", \"SE\":\"Sweden\", \"SUHH\":\"Soviet Union\", \n",
    "            \"TR\":\"Turkey\", \"TW\":\"Taiwan\", \"UK\":\"United Kingdom\",  \"USA\":\"United States\", \"US\":\"United States\",\"USSR\":\"Soviet Union\", \n",
    "            \"XWG\":\"West Germany\", \"YUCS\":\"Yugoslavia\"}\n",
    "\n",
    "# film_df.info()\n",
    "for idx in range(0, len(film_df)):\n",
    "    if type(film_df.loc[idx, \"Country\"]) == str:\n",
    "        for code, lang in lang_dict.items():\n",
    "            if lang not in film_df.loc[idx, \"Country\"]:\n",
    "                film_df.loc[idx, \"Country\"] = film_df.loc[idx, \"Country\"].replace(code, lang)\n",
    "            else:\n",
    "                film_df.loc[idx, \"Country\"] = film_df.loc[idx, \"Country\"].replace(code, \"\").replace(\" ,\", \"\").replace(\", \", \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede57e2e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b54863",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp = dt.datetime.now(dt.timezone.utc)\n",
    "timestamp = f\"{tmp.year:04}{tmp.month:02}{tmp.day:02}\"\n",
    "film_df.to_csv(f\"{source_type}_{len(film_df)}_{timestamp}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.81612,
   "end_time": "2023-07-19T09:52:12.234025",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-19T09:51:58.417905",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
